{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of ultimate student challenge hosted by analytics vidhya. In this notebook, I have used various machine learning and intuitive methods to improve training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import pandas to manipulate dataframes\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read train and test data\n",
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Park_ID', 'Date', 'Direction_Of_Wind', 'Average_Breeze_Speed',\n",
       "       'Max_Breeze_Speed', 'Min_Breeze_Speed', 'Var1',\n",
       "       'Average_Atmospheric_Pressure', 'Max_Atmospheric_Pressure',\n",
       "       'Min_Atmospheric_Pressure', 'Min_Ambient_Pollution',\n",
       "       'Max_Ambient_Pollution', 'Average_Moisture_In_Park',\n",
       "       'Max_Moisture_In_Park', 'Min_Moisture_In_Park', 'Location_Type',\n",
       "       'Footfall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Park_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Direction_Of_Wind</th>\n",
       "      <th>Average_Breeze_Speed</th>\n",
       "      <th>Max_Breeze_Speed</th>\n",
       "      <th>Min_Breeze_Speed</th>\n",
       "      <th>Var1</th>\n",
       "      <th>Average_Atmospheric_Pressure</th>\n",
       "      <th>Max_Atmospheric_Pressure</th>\n",
       "      <th>Min_Atmospheric_Pressure</th>\n",
       "      <th>Min_Ambient_Pollution</th>\n",
       "      <th>Max_Ambient_Pollution</th>\n",
       "      <th>Average_Moisture_In_Park</th>\n",
       "      <th>Max_Moisture_In_Park</th>\n",
       "      <th>Min_Moisture_In_Park</th>\n",
       "      <th>Location_Type</th>\n",
       "      <th>Footfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3311712</td>\n",
       "      <td>12</td>\n",
       "      <td>01-09-1990</td>\n",
       "      <td>194.0</td>\n",
       "      <td>37.24</td>\n",
       "      <td>60.8</td>\n",
       "      <td>15.2</td>\n",
       "      <td>92.1300</td>\n",
       "      <td>8225.0</td>\n",
       "      <td>8259.0</td>\n",
       "      <td>8211.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3311812</td>\n",
       "      <td>12</td>\n",
       "      <td>02-09-1990</td>\n",
       "      <td>285.0</td>\n",
       "      <td>32.68</td>\n",
       "      <td>60.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>14.1100</td>\n",
       "      <td>8232.0</td>\n",
       "      <td>8280.0</td>\n",
       "      <td>8205.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3311912</td>\n",
       "      <td>12</td>\n",
       "      <td>03-09-1990</td>\n",
       "      <td>319.0</td>\n",
       "      <td>43.32</td>\n",
       "      <td>60.8</td>\n",
       "      <td>15.2</td>\n",
       "      <td>35.6900</td>\n",
       "      <td>8321.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>8283.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3312012</td>\n",
       "      <td>12</td>\n",
       "      <td>04-09-1990</td>\n",
       "      <td>297.0</td>\n",
       "      <td>25.84</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>8379.0</td>\n",
       "      <td>8396.0</td>\n",
       "      <td>8358.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3312112</td>\n",
       "      <td>12</td>\n",
       "      <td>05-09-1990</td>\n",
       "      <td>207.0</td>\n",
       "      <td>28.88</td>\n",
       "      <td>45.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>8372.0</td>\n",
       "      <td>8393.0</td>\n",
       "      <td>8335.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Park_ID        Date  Direction_Of_Wind  Average_Breeze_Speed  \\\n",
       "0  3311712       12  01-09-1990              194.0                 37.24   \n",
       "1  3311812       12  02-09-1990              285.0                 32.68   \n",
       "2  3311912       12  03-09-1990              319.0                 43.32   \n",
       "3  3312012       12  04-09-1990              297.0                 25.84   \n",
       "4  3312112       12  05-09-1990              207.0                 28.88   \n",
       "\n",
       "   Max_Breeze_Speed  Min_Breeze_Speed     Var1  Average_Atmospheric_Pressure  \\\n",
       "0              60.8              15.2  92.1300                        8225.0   \n",
       "1              60.8               7.6  14.1100                        8232.0   \n",
       "2              60.8              15.2  35.6900                        8321.0   \n",
       "3              38.0               7.6   0.0249                        8379.0   \n",
       "4              45.6               7.6   0.8300                        8372.0   \n",
       "\n",
       "   Max_Atmospheric_Pressure  Min_Atmospheric_Pressure  Min_Ambient_Pollution  \\\n",
       "0                    8259.0                    8211.0                   92.0   \n",
       "1                    8280.0                    8205.0                  172.0   \n",
       "2                    8355.0                    8283.0                  236.0   \n",
       "3                    8396.0                    8358.0                  272.0   \n",
       "4                    8393.0                    8335.0                  236.0   \n",
       "\n",
       "   Max_Ambient_Pollution  Average_Moisture_In_Park  Max_Moisture_In_Park  \\\n",
       "0                  304.0                     255.0                 288.0   \n",
       "1                  332.0                     252.0                 297.0   \n",
       "2                  292.0                     219.0                 279.0   \n",
       "3                  324.0                     225.0                 261.0   \n",
       "4                  332.0                     234.0                 273.0   \n",
       "\n",
       "   Min_Moisture_In_Park  Location_Type  Footfall  \n",
       "0                 222.0              3      1406  \n",
       "1                 204.0              3      1409  \n",
       "2                 165.0              3      1386  \n",
       "3                 192.0              3      1365  \n",
       "4                 183.0              3      1413  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are number of things we can do to improve our training data:  \n",
    "1. We can not handle dates directly in machine learning algorithms so we can split dates into day, month and year\n",
    "2. ID is just increasing number and it is record identifier. This is not an important feature so discard it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split date into day,month and year in train and test data\n",
    "train_dtObj = pd.DatetimeIndex(train_data['Date'])\n",
    "train_data['year'] = train_dtObj.year\n",
    "train_data['month'] = train_dtObj.month\n",
    "train_data['day'] = train_dtObj.day\n",
    "\n",
    "test_dtObj = pd.DatetimeIndex(test_data['Date'])\n",
    "test_data['year'] = test_dtObj.year\n",
    "test_data['month'] = test_dtObj.month\n",
    "test_data['day'] = test_dtObj.day\n",
    "\n",
    "# Save Ids of test data set\n",
    "IDs = test_data.ID\n",
    "\n",
    "# Drop ID and original date columns\n",
    "train_data = train_data.drop('ID',1)\n",
    "train_data = train_data.drop('Date',1)\n",
    "test_data = test_data.drop(\"ID\",1)\n",
    "test_data = test_data.drop(\"Date\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Park_ID                             0\n",
       "Direction_Of_Wind                3931\n",
       "Average_Breeze_Speed             3931\n",
       "Max_Breeze_Speed                 3936\n",
       "Min_Breeze_Speed                 3934\n",
       "Var1                             8282\n",
       "Average_Atmospheric_Pressure    40195\n",
       "Max_Atmospheric_Pressure        40195\n",
       "Min_Atmospheric_Pressure        40195\n",
       "Min_Ambient_Pollution           31645\n",
       "Max_Ambient_Pollution           31645\n",
       "Average_Moisture_In_Park           40\n",
       "Max_Moisture_In_Park               40\n",
       "Min_Moisture_In_Park               40\n",
       "Location_Type                       0\n",
       "Footfall                            0\n",
       "year                                0\n",
       "month                               0\n",
       "day                                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Park_ID                             0\n",
       "Direction_Of_Wind                1493\n",
       "Average_Breeze_Speed             1493\n",
       "Max_Breeze_Speed                 1493\n",
       "Min_Breeze_Speed                 1493\n",
       "Var1                             2920\n",
       "Average_Atmospheric_Pressure    13173\n",
       "Max_Atmospheric_Pressure        13173\n",
       "Min_Atmospheric_Pressure        13173\n",
       "Min_Ambient_Pollution            9655\n",
       "Max_Ambient_Pollution            9655\n",
       "Average_Moisture_In_Park           39\n",
       "Max_Moisture_In_Park               39\n",
       "Min_Moisture_In_Park               39\n",
       "Location_Type                       0\n",
       "year                                0\n",
       "month                               0\n",
       "day                                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many records with missing values in train data and test data. We can not simply discard it as it can hurt performance badly. We can use some intuitive methods and machine learning to predict missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have following 6 different approach to estimate missing values**\n",
    "\n",
    "1. Generalized linear models\n",
    "2. Group by month and fill missing values with mean of each columns\n",
    "3. K-Nearest neighbour regressor\n",
    "4. Random forest regressor\n",
    "5. Gradient boosting regressor\n",
    "6. Average all models to reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import machine learning algorithms to predict missing values\n",
    "from sklearn.linear_model import LinearRegression,RidgeCV,LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_missing_values_using_all_features(_data,model):\n",
    "    data = _data.copy()\n",
    "    columns_with_NA = data.loc[:,pd.isnull(data).sum() > 0].columns\n",
    "    for column_name in columns_with_NA:\n",
    "        if(data[column_name].isnull().sum() == 0):\n",
    "            break\n",
    "        #print(column_name)\n",
    "        data[column_name] = data.groupby([\"year\",\"month\",\"Location_Type\",\"Park_ID\"])[column_name].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    columns_with_NA = data.loc[:,pd.isnull(data).sum() > 0].columns\n",
    "    for missingF in columns_with_NA:\n",
    "        good_features = data.loc[:,pd.isnull(data).sum() == 0].columns\n",
    "\n",
    "        if(data[missingF].isnull().sum() == 0):\n",
    "            break\n",
    "        print(\"Finding missing values for \",missingF)\n",
    "        tr_data = data[~data[missingF].isnull()]\n",
    "        ts_data = data[data[missingF].isnull()]\n",
    "        x_train = tr_data[good_features]\n",
    "        y = tr_data[missingF]\n",
    "        x_test = ts_data[good_features]\n",
    "        model.fit(x_train,y)\n",
    "        data.loc[data[missingF].isnull(),missingF] = model.predict(x_test)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_missing_values_using_categorical_features(_data,model):\n",
    "    data = _data.copy()\n",
    "\n",
    "    columns_with_NA = data.loc[:,pd.isnull(data).sum() > 0].columns\n",
    "    for missingF in columns_with_NA:\n",
    "        good_features = [\"year\",\"month\",\"Location_Type\",\"Park_ID\",\"day\"]\n",
    "\n",
    "        if(data[missingF].isnull().sum() == 0):\n",
    "            break\n",
    "        print(\"Finding missing values for \",missingF)\n",
    "        tr_data = data[~data[missingF].isnull()]\n",
    "        ts_data = data[data[missingF].isnull()]\n",
    "        x_train = tr_data[good_features]\n",
    "        y = tr_data[missingF]\n",
    "        x_test = ts_data[good_features]\n",
    "        model.fit(x_train,y)\n",
    "        data.loc[data[missingF].isnull(),missingF] = model.predict(x_test)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def impute_missing_values_combined(_data,model):\n",
    "    data = _data.copy()\n",
    "    columns_with_NA = data.loc[:,pd.isnull(data).sum() > 0].columns\n",
    "    for column_name in columns_with_NA:\n",
    "        if(data[column_name].isnull().sum() == 0):\n",
    "            break\n",
    "        #print(column_name)\n",
    "        data[column_name] = data.groupby([\"year\",\"month\",\"Location_Type\",\"Park_ID\"])[column_name].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    #Predict \"Var1\"\n",
    "    good_features = data.loc[:,pd.isnull(data).sum() == 0].columns\n",
    "    if(data[\"Var1\"].isnull().sum() != 0):\n",
    "        #print(\"Finding missing values for \",missingF)\n",
    "        tr_data = data[~data[\"Var1\"].isnull()]\n",
    "        ts_data = data[data[\"Var1\"].isnull()]\n",
    "        x_train = tr_data[good_features]\n",
    "        y = tr_data[\"Var1\"]\n",
    "        x_test = ts_data[good_features]\n",
    "        model.fit(x_train,y)\n",
    "        data.loc[data[\"Var1\"].isnull(),\"Var1\"] = model.predict(x_test)\n",
    "    \n",
    "    columns_with_NA = data.loc[:,pd.isnull(data).sum() > 0].columns\n",
    "    #print(columns_with_NA)\n",
    "    for missingF in columns_with_NA:\n",
    "        good_features = data.loc[:,pd.isnull(data).sum() == 0].columns\n",
    "\n",
    "        if(data[missingF].isnull().sum() == 0):\n",
    "            break\n",
    "        #print(\"Finding missing values for \",missingF)\n",
    "        tr_data = data[~data[missingF].isnull()]\n",
    "        ts_data = data[data[missingF].isnull()]\n",
    "        x_train = tr_data[good_features]\n",
    "        y = tr_data[missingF]\n",
    "        x_test = ts_data[good_features]\n",
    "        model.fit(x_train,y)\n",
    "        data.loc[data[missingF].isnull(),missingF] = model.predict(x_test)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featureset 1 - Linear, Ridge, Lasso average using combined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in featureset 1: in train -  0  in test -  0\n"
     ]
    }
   ],
   "source": [
    "# Initialize regression models with paramaters\n",
    "ridge_model = RidgeCV(alphas=[1,0.1,0.01,0.001,0.0001])\n",
    "lasso_model = LassoCV(alphas=[1,0.1,0.01,0.001,0.0001])\n",
    "reg_model = LinearRegression()\n",
    "\n",
    "# Impute missing values using approach 3: both categorical and continuous variables (Predict var 1 first)\n",
    "tr11 = impute_missing_values_combined(train_data,ridge_model)\n",
    "tr12 = impute_missing_values_combined(train_data,lasso_model)\n",
    "tr13 = impute_missing_values_combined(train_data,reg_model)\n",
    "\n",
    "ts11 = impute_missing_values_combined(test_data,ridge_model)\n",
    "ts12 = impute_missing_values_combined(test_data,lasso_model)\n",
    "ts13 = impute_missing_values_combined(test_data,reg_model)\n",
    "\n",
    "# Average all regression models to reduce variance\n",
    "tr1 = (tr11 + tr12 + tr13)/3\n",
    "ts1 = (ts11 + ts12 + ts13)/3\n",
    "\n",
    "# Print number of missing values to make sure there are no missing values\n",
    "print(\"Missing values in featureset 1: in train - \",tr1.isnull().sum().sum(),\" in test - \",ts1.isnull().sum().sum())\n",
    "\n",
    "# Save featureset 1 into filesystem\n",
    "tr1.to_csv(\"featuresets/tr1.csv\")\n",
    "ts1.to_csv(\"featuresets/ts1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featureset 2 - Group by month and average columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in featureset 2: in train -  0  in test -  0\n"
     ]
    }
   ],
   "source": [
    "# Group train and test data by month and fill missing values with the mean of entire column\n",
    "tr2 = train_data.groupby([\"month\"]).transform(lambda x: x.fillna(x.mean()))\n",
    "ts2 = test_data.groupby([\"month\"]).transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Add month column as it is \n",
    "tr2[\"month\"] = train_data[\"month\"]\n",
    "ts2[\"month\"] = test_data[\"month\"]\n",
    "\n",
    "# Print number of missing values to make sure there are no missing values\n",
    "print(\"Missing values in featureset 2: in train - \",tr2.isnull().sum().sum(),\" in test - \",ts2.isnull().sum().sum())\n",
    "\n",
    "# Save featureset 2 into filesystem\n",
    "tr2.to_csv(\"featuresets/tr2.csv\")\n",
    "ts2.to_csv(\"featuresets/ts2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featureset 3 - KNN with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4d3bd7061093>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Impute missing values using approach 1: all features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtr3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimpute_missing_values_using_all_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mknr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mts3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimpute_missing_values_using_all_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mknr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e7289ee619a2>\u001b[0m in \u001b[0;36mimpute_missing_values_using_all_features\u001b[1;34m(_data, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#print(column_name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Location_Type\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Park_ID\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcolumns_with_NA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/achal/anaconda3/lib/python3.5/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2748\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2749\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2751\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/achal/anaconda3/lib/python3.5/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   2744\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2746\u001b[1;33m         \u001b[0mwrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2748\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e7289ee619a2>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#print(column_name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Location_Type\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Park_ID\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcolumns_with_NA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/achal/anaconda3/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   2293\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2294\u001b[0m                                           \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2295\u001b[1;33m                                           **kwargs)\n\u001b[0m\u001b[0;32m   2296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2297\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shift'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/achal/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_fill_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/achal/anaconda3/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize nearest neighbour regressor with paramaters\n",
    "knr = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Impute missing values using approach 1: all features\n",
    "tr3 = impute_missing_values_using_all_features(train_data,knr)\n",
    "ts3 = impute_missing_values_using_all_features(test_data,knr)\n",
    "\n",
    "# Print number of missing values to make sure there are no missing values\n",
    "print(\"Missing values in featureset 3: in train - \",tr3.isnull().sum().sum(),\" in test - \",ts3.isnull().sum().sum())\n",
    "\n",
    "# Save featureset 3 into filesystem\n",
    "tr3.to_csv(\"featuresets/tr3.csv\")\n",
    "ts3.to_csv(\"featuresets/ts3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featureset 4 - Randomforest regressor with good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding missing values for  Direction_Of_Wind\n",
      "Finding missing values for  Average_Breeze_Speed\n",
      "Finding missing values for  Max_Breeze_Speed\n",
      "Finding missing values for  Min_Breeze_Speed\n",
      "Finding missing values for  Var1\n",
      "Finding missing values for  Average_Atmospheric_Pressure\n",
      "Finding missing values for  Max_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Ambient_Pollution\n",
      "Finding missing values for  Max_Ambient_Pollution\n",
      "Finding missing values for  Average_Moisture_In_Park\n",
      "Finding missing values for  Max_Moisture_In_Park\n",
      "Finding missing values for  Min_Moisture_In_Park\n",
      "Finding missing values for  Direction_Of_Wind\n",
      "Finding missing values for  Average_Breeze_Speed\n",
      "Finding missing values for  Max_Breeze_Speed\n",
      "Finding missing values for  Min_Breeze_Speed\n",
      "Finding missing values for  Var1\n",
      "Finding missing values for  Average_Atmospheric_Pressure\n",
      "Finding missing values for  Max_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Ambient_Pollution\n",
      "Finding missing values for  Max_Ambient_Pollution\n",
      "Finding missing values for  Average_Moisture_In_Park\n",
      "Finding missing values for  Max_Moisture_In_Park\n",
      "Finding missing values for  Min_Moisture_In_Park\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize random forest regressor with paramaters\n",
    "rfr = RandomForestRegressor(n_estimators=70)\n",
    "\n",
    "# Impute missing values using approach 2: categorical features\n",
    "tr4 = impute_missing_values_using_categorical_features(train_data,rfr)\n",
    "ts4 = impute_missing_values_using_categorical_features(test_data,rfr)\n",
    "\n",
    "# Print number of missing values to make sure there are no missing values\n",
    "print(\"Missing values in featureset 4: in train - \",tr4.isnull().sum().sum(),\" in test - \",ts4.isnull().sum().sum())\n",
    "\n",
    "# Save featureset 4 into filesystem\n",
    "tr4.to_csv(\"featuresets/tr4.csv\")\n",
    "ts4.to_csv(\"featuresets/ts4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featureset 5 - GBM with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding missing values for  Direction_Of_Wind\n",
      "Finding missing values for  Average_Breeze_Speed\n",
      "Finding missing values for  Max_Breeze_Speed\n",
      "Finding missing values for  Min_Breeze_Speed\n",
      "Finding missing values for  Var1\n",
      "Finding missing values for  Average_Atmospheric_Pressure\n",
      "Finding missing values for  Max_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Ambient_Pollution\n",
      "Finding missing values for  Max_Ambient_Pollution\n",
      "Finding missing values for  Average_Moisture_In_Park\n",
      "Finding missing values for  Max_Moisture_In_Park\n",
      "Finding missing values for  Min_Moisture_In_Park\n",
      "Finding missing values for  Direction_Of_Wind\n",
      "Finding missing values for  Average_Breeze_Speed\n",
      "Finding missing values for  Max_Breeze_Speed\n",
      "Finding missing values for  Min_Breeze_Speed\n",
      "Finding missing values for  Var1\n",
      "Finding missing values for  Average_Atmospheric_Pressure\n",
      "Finding missing values for  Max_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Atmospheric_Pressure\n",
      "Finding missing values for  Min_Ambient_Pollution\n",
      "Finding missing values for  Max_Ambient_Pollution\n",
      "Finding missing values for  Average_Moisture_In_Park\n",
      "Finding missing values for  Max_Moisture_In_Park\n",
      "Finding missing values for  Min_Moisture_In_Park\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize gbm regressor with paramaters\n",
    "gbm = GradientBoostingRegressor(n_estimators=200,learning_rate=0.2,max_depth=4, min_samples_split=1)\n",
    "\n",
    "# Impute missing values using approach 2: categorical features\n",
    "tr5 = impute_missing_values_using_categorical_features(train_data,gbm)\n",
    "ts5 = impute_missing_values_using_categorical_features(test_data,gbm)\n",
    "\n",
    "# Print number of missing values to make sure there are no missing values\n",
    "print(\"Missing values in featureset 5: in train - \",tr5.isnull().sum().sum(),\" in test - \",ts5.isnull().sum().sum())\n",
    "\n",
    "# Save featureset 6 into filesystem\n",
    "tr5.to_csv(\"featuresets/tr5.csv\")\n",
    "ts5.to_csv(\"featuresets/ts5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featureset 6 - Average all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "# Average featureset 1 to 5\n",
    "tr6 = (tr1 + tr2 + tr3 + tr4 + tr5)/5\n",
    "ts6 = (ts1 + ts2 + ts3 + ts4 + ts5)/5\n",
    "\n",
    "# Print number of missing values to make sure there are no missing values\n",
    "print(\"Missing values in featureset 6: in train - \",tr6.isnull().sum().sum(),\" in test - \",ts6.isnull().sum().sum())\n",
    "\n",
    "# Save featureset 6 into filesystem\n",
    "tr6.to_csv(\"featuresets/tr6.csv\")\n",
    "ts6.to_csv(\"featuresets/ts6.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
